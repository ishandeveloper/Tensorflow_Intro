{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "#### Author : ishandeveloper\n",
    "\n",
    "In the previous example, we saw how we could create a neural network called a deep neural network to pattern match a set of images of fashion items to labels. \n",
    "\n",
    "Now, one of the things that we have have seen when we looked at the images is that there's a lot of wasted space in each image. While there are only 784 pixels, it will be interesting to see if there was a way that we could condense the image down to the important features that distinguish what makes it a shoe, or a handbag, or a shirt. That's where convolutions come in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions in a very basic way can be inferred to as applying filters on an image to enhance some of it's distinguishing features. For example a filter that helps to emphasize all the vertical lines in an image and another one for horizontal lines etc.\n",
    "\n",
    "#### When these convolutions are combined with something called pooling, they can become really powerful. \n",
    "\n",
    "## Pooling\n",
    "Simply, pooling is a way of compressing an image. A quick and easy way to do this, is to go over the image of four pixels at a time, i.e, the current pixel and its neighbors underneath and to the right of it. Of these four, pick the biggest value and keep just that. This will preserve the features that were highlighted by the convolution, while simultaneously quartering the size of the image. We have the horizontal and vertical axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, Let's <br>\n",
    "<b>Get The Dependencies</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here's our code from the earlier example, where we defined out a neural network to have an input layer in the shape of our data, and output layer in the shape of the number of categories we're trying to define, and a hidden layer in the middle. The Flatten takes our square 28 by 28 images and turns them into a one dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# Splitting Dataset\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu ),\n",
    "    keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to evaluate the above neural network once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5019 - accuracy: 0.8235\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3764 - accuracy: 0.8645\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3379 - accuracy: 0.8774s - loss:\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.3115 - accuracy: 0.8876\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.2968 - accuracy: 0.8915\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.3570 - accuracy: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3570241296052933, 0.8727]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the above Neural Network gives us an accuracy of <b>88%</b>. Later on, we'll check how much difference does adding a CNN makes to the above accuracy.\n",
    "\n",
    "To add CNN to this, we'll simply add all other different layers. But, first let's reshape our dataset so that we can fit it into our model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# Splitting Dataset\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let's add CNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu',input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu ),\n",
    "    keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that the last three lines are the same, the Flatten, the Dense hidden layer with 128 neurons, and the Dense output layer with 10 neurons. What's different is what has been added on top of this. Let's take a look at this, line by line.\n",
    "\n",
    "Here we're specifying the first convolution. We're asking keras to generate 64 filters for us. These filters are 3 by 3, their activation is relu, which means the negative values will be thrown way, and finally the input shape is as before, the 28 by 28. That extra 1 just means that we are tallying using a single byte for color depth. As we saw before our image is our gray scale, so we just use one byte.\n",
    "\n",
    "The next line of code will then create a pooling layer. It's max-pooling because we're going to take the maximum value. We're saying it's a two-by-two pool, so for every four pixels, the biggest one will survive as shown earlier. We then add another convolutional layer, and another max-pooling layer so that the network can learn another set of convolutions on top of the existing one, and then again, pool to reduce the size. So, by the time the image gets to the flatten to go into the dense layers, it's already much smaller. It's being quartered, and then quartered again. So, its content has been greatly simplified, the goal being that the convolutions will filter it to the features that determine the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really useful method on the model is the model.summary method. This allows us to inspect the layers of the model, and see the journey of the image through the convolutions, and here is the output. It's a nice table showing us the layers, and some details about them including the output shape.\n",
    "\n",
    "It's important to keep an eye on the output shape column. When you first look at this, it can be a little bit confusing and feel like a bug. After all, isn't the data 28 by 28, so y is the output, 26 by 26. The key to this is remembering that the filter is a three by three filter.\n",
    "\n",
    "You can't calculate the filter for the pixel in the top left, because it doesn't have any neighbors above it or to its left. In a similar fashion, the next pixel to the right won't work either because it doesn't have any neighbors above it. So, logically, the first pixel that you can do calculations on is at location (2,2) leaving a 1 pixel margin on all sides of the image and this resulting in <b>26 by 26</b> Output Size.\n",
    "\n",
    "Now, Let's Evaluate the model again.\n",
    "\n",
    "<b>This process might take a few minutes as it is highly GPU intensive.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 73s 1ms/sample - loss: 0.4384 - accuracy: 0.8406\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.2945 - accuracy: 0.8914\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 102s 2ms/sample - loss: 0.2483 - accuracy: 0.9083\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 97s 2ms/sample - loss: 0.2155 - accuracy: 0.9191\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 90s 2ms/sample - loss: 0.1878 - accuracy: 0.9301\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 0.2560 - accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2560009437441826, 0.91]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
