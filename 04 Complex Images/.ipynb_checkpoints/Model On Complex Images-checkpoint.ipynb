{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Complex Images (Self-Defined Datasets)\n",
    "\n",
    "#### Author : ishandeveloper\n",
    "\n",
    "In previous examples we built an image classifier that worked using a deep neural network and saw how to improve its performance by adding convolutions. One limitation though was that it used a dataset of very uniform images. Images of clothing that was staged and framed in 28 by 28. \n",
    "\n",
    "But what happens when we use larger images and where the feature might be in different locations? For example, how about a few images of horses and humans? They have different sizes and different aspect ratios. The subject can be in different locations. In some cases, there may even be multiple subjects. In addition to that, the earlier examples with a fashion data used a built-in dataset. All of the data was handily split into training and test sets for us and labels were available. In many scenarios, that's not going to be the case and we'll have to do it for yourself.\n",
    "\n",
    "So, To make it easier for us we'll use Tensorflow APIs and Take a look at the <br>\n",
    "### <b>Image generator in TensorFlow</b>\n",
    "\n",
    "Let's Jump right into the code to see it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to import the <b>ImageDataGenerator</b> from tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then instantiate an image generator like this and pass rescale to it to normalize the data. We can then call the flow from directory method on it to get it to load images from that directory and its sub-directories.\n",
    "\n",
    "The nice thing about this code is that the <b>images are resized</b> for you as they're loaded. So you don't need to preprocess thousands of images on your file system.The images will be loaded for training and validation in <b>batches</b> where it's more efficient than doing it one by one.Finally, there's the class mode. \n",
    "\n",
    "Now, this is a <b>binary classifier</b> i.e. it picks between two different things;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                train_dir,\n",
    "                target_size=(300,300),\n",
    "                batch_size=128,\n",
    "                class_mode='binary' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same steps for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                validation_dir,\n",
    "                target_size=(300,300),\n",
    "                batch_size=128,\n",
    "                class_mode='binary' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here's the code. As you can see, it's the sequential as before with convolutions and pooling before we get to the dense layers at the bottom. \n",
    "\n",
    "But there are a few differences. \n",
    "\n",
    "First of all, there are <b>three sets of convolution pooling layers</b> at the top. This reflects the higher complexity and size of the images.\n",
    "\n",
    "Another thing to pay attention to is the input shape. We resize their images to be 300 by 300 as they were loaded, but they're also color images. So there are three bytes per pixel. One for R,G,B each and that's a common 24-bit color pattern. \n",
    "\n",
    "Now, The output layer has also changed. Because previously, we had one neuron per class, but now there's only one neuron for two classes. That's because we're using a different activation function where <b>sigmoid</b> is great for binary classification, where one class will tend towards zero and the other class tending towards one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(16, (3,3), activation='relu',input_shape=(300, 300, 3)),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu ),\n",
    "    keras.layers.Dense(1,  activation=tf.nn.sigmoid)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we take a look at our model summary, we can see the journey of the image data through the convolutions The 300 by 300 becomes 298 by 298 after the three by three filter, it gets pulled to 149 by 149 which in turn gets reduced to 73 by 73 after the filter that then gets pulled to 35 by 35, this will then get flattened, so 64 convolutions that are 35 squared and shape will get fed into the DNN. If you multiply 35 by 35 by 64, you get 78,400, and that's the shape of the data once it comes out of the convolutions. If we had just fed raw 300 by 300 images without the convolutions, that would be over 900,000 values. So we've already reduced it quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compile the model and, as always, we have a loss function and an optimizer. \n",
    "\n",
    "When classifying the ten items of fashion, we might remember that our loss function was a categorical cross entropy. But because we're doing a binary choice here, let's pick a <b>binary_crossentropy</b> instead. Also, earlier we used an Adam optimizer but we can also use RMSprop, where we can adjust the learning rate to experiment with performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=RMSprop(lr=0.001),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve designed the neural network to classify Horses or Humans, the next step is to train it from data that’s on the file system, which can be read by generators. To do this, <b>we don’t use model.fit</b> as earlier, but a new method call: <b>model.fit_generator</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=8,\n",
    "        epochs=15,\n",
    "        validation_data=validation_data_generator,\n",
    "        validation_steps=2,\n",
    "        verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at each parameter in detail. \n",
    "The first parameter is the training generator that we set up earlier. There are 1,024 images in the training directory, so we're loading them in 128 at a time. So in order to load them all, we need to do 8 batches. So we set the steps_per_epoch to cover that.\n",
    "\n",
    "Now we specify the validation set that comes from the validation_generator that we also created earlier. It had 256 images, and we wanted to handle them in batches of 32, so we will do 8 steps.\n",
    "\n",
    "And the verbose parameter specifies how much to display while training is going on. With verbose set to 2, we'll get a little less animation hiding the epoch progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's first Import image from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now to predict something from the above, we can use this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='' # Location Of Image From Which Result Is To Be Predicted\n",
    "img = image.load_img(path, target_size=(300, 300))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "images = np.vstack([x])\n",
    "classes = model.predict(images, batch_size=10)\n",
    "\n",
    "print(classes[0])\n",
    "if(classes[0]>0.5):\n",
    "    print(path + ' is a human')\n",
    "else:\n",
    "    print(path + ' is a horse')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
